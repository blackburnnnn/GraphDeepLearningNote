### mac本地环境配置

- anaconda环境已经安装好的，直接进入点击jupyter就可以本地启动了，内核也和本地的对应

  ![image-20230227180844899](/Users/leizhenhao/Library/Application Support/typora-user-images/image-20230227180844899.png)

- 虽然叫python375，但是mac python只能支持3.9版本以上的

- Jupyter Notebook操作

  - 终端启动命令(不用打开Anaconda) `/Users/leizhenhao/opt/anaconda3/bin/jupyter_mac.command ; exit;`

### linux command

- 删除文件夹 `rm -rf {文件夹名称}`
- 查看磁盘占用 `df   -h`
- 查看当前路径以及子级文件和文件夹大小 `du -h --max-depth=1`

### Numpy&Scipy

- [`import scipy.sparse as sp` 构建稀疏矩阵与复原](https://blog.csdn.net/Xu_JL1997/article/details/83036442)

  - [使用原因](https://baijiahao.baidu.com/s?id=1666715444573651466&wfr=spider&for=pc)，而且**直观上cora的邻接矩阵就非常稀疏**

    <img src="/Users/leizhenhao/Library/Application Support/typora-user-images/image-20230117115337114.png" alt="image-20230117115337114" style="zoom:80%;" />

  - 本地测试代码

    ```python
    A1 = np.array([[1, 2, 0, 0], [0, 3, 4, 0], [0, 0, 5, 6], [7, 0, 8, 9]])
    A2 = sp.csc_matrix(A1)  # 按列存储
    A3 = sp.csr_matrix(A1)  # 按行存储
    print(A1)
    print('列存:', A2)
    print('行存:', A3)
    print('按列存储的所有非零元素:', A2.data)  # 按列存储所有非零元素
    print('按列存储的所有非零元素', A3.data)  # 按行存储所有非零元素
    print("将按列存储的A2转化为按行存储的B1")
    B1 = sp.csr_matrix(A2)
    print('转化为行存:', B1)
    print('转化后按行存储的所有非零元素:', B1.data)
    # 将稀疏矩阵转化为原始矩阵
    print(B1.todense())
    print(A2.todense())
    print(A3.todense())
    ```

  - **`.todense()` 返回numpy.matrix()类型，matrix只能表示二维数据,而ndarray可以表示N维数据**

- numpy基本数据类型：ndarray

  - `np.unique(label)` 查看有哪些不同的值
  - `np.squeeze` 就是把多余的中括号去掉
  - `.shape[0]` 获取矩阵第0维度
  - `np.arange()` 生成指定排列`[]`

### 深度学习基础

- [构建模型三要素与权重随机化](https://www.cnblogs.com/PythonLearner/p/13587092.html)

- [`torch.nn.Linear()`原理](https://blog.csdn.net/qq_35037684/article/details/121624295)，就是一个矩阵，`.weight.data`获取该权重矩阵的具体值

- **损失函数基础**

  1. softmax计算公式 $P\left(S_i\right)=\frac{e^{g_i}}{\sum_k^n e^{g_k}}$

  2. 不使用MSE损失函数是因为梯度下降时，使用MSE会导致开始训练时学习速率很慢

  3. 交叉熵损失函数通过公式基于每个预测的样本计算其loss，然后对所有样本loss求平均，且函数性质在梯度下降时非常好

     <img src="/Users/leizhenhao/Library/Application Support/typora-user-images/image-20230211120819643.png" alt="image-20230211120819643" style="zoom:35%;" />

  4. 由于交叉熵损失函数涉及到计算每个类别的概率，所以交叉熵几乎每次都和sigmoid(或softmax)函数一起出现，整体流程为如下：

     1. **神经网络最后一层得到每个类别的得分scores(也叫logits)**
     2. **该得分经过sigmoid(或softmax)函数获得概率输出**
     3. **(关键点)模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算**

  5. - 二分类情况

       <img src="https://pic1.zhimg.com/80/v2-d44fea1bda9338eaabf8e96df099981c_1440w.webp" alt="img" style="zoom:50%;" />

     - 多分类情况

       <img src="https://pic2.zhimg.com/80/v2-08a626d27078d36541ad0b02d560efa5_1440w.webp" alt="img" style="zoom:50%;" />
